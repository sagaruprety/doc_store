{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e63f50",
   "metadata": {},
   "source": [
    "### The [langchain framework](https://python.langchain.com/docs/get_started/introduction.html) makes it easy to use LLMs as [agents](https://python.langchain.com/docs/modules/agents/) capable of making decisions. Furthermore, these agents can be equipped with a variety of [tools](https://python.langchain.com/docs/modules/agents/tools/) which implement different functionalities. LLM agents can be given access of a combination of such tools. The decision to use a particular tool as part of solving a particular task is based on the language understanding ability of the LLMs. \n",
    "\n",
    "\n",
    "### In this post, we will explore the power of Langchain agents and their application in question answering over a text document store. Specifically, we will dive into creating a Jupyter Notebook that demonstrates the integration of Large Language model (LLM) with external text documents and a search engine, enabling seamless information retrieval. We'll also discuss the challenges encountered during the planning phase and how to overcome them. In particular we discuss how to make agents use the tools in a particular order, whenever they decide to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe8870",
   "metadata": {},
   "source": [
    "### We start with installing and importing relevant libraries and setting up the openAI and SERP API keys. You can use an open source model like Llama2 instead. Langchain serves the functionality to load it via llama-cpp-python library.\n",
    "\n",
    "### You can clone the git repo: and run this notebook from within the repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8acb96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have cloned the git repo, the requirements.txt file will list all required libraries\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c23659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import load_tools\n",
    "from langchain import OpenAI, SerpAPIWrapper\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"b4b1d59017daf45a63fcad672a4b4ce8e23bb3ad7f9cf7365956191e23c302a1\"\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898760d",
   "metadata": {},
   "source": [
    "## Connecting to a Document Store\n",
    "\n",
    "### Now that you have the OpenAI LLM model ready, let's move on to connecting it to documents and fetching data. To achieve this, we'll make use of Directory document loader available in Langchain. I have some text files stored in another directory in the same git repo. This can be replaced by any document store as a wiki/knowledge base, e.g. Confluence, Google Drive, S3 bucket, Any database, etc. Langchain has hunderds of [connectors](https://python.langchain.com/docs/modules/data_connection/document_loaders/) to third party document storage tools.\n",
    "\n",
    "\n",
    "### The documents I have used in this document store are notes of some LLM based papers I took while reading them, most of the time copying and pasting the text from those papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b5668f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='ChatDoctor Paper\\n\\n\\nfine- tuned model based on 100k real-world patient-physician conversations from an online medical consultation site. \\n\\nAlso added  autonomous knowledge retrieval capabilities to our ChatDoctor, for ex- ample, Wikipedia or a database as a knowledge brain. \\n\\nCollected about 100k real doctor-patient conversations from an online medical consultation website HealthCareMagic1. We filtered these data both manually and automatically, removed the identity information of the doctor and patient, and used language tools to correct grammatical errors, and we named this dataset HealthCareMagic-100k.\\n\\nIn addition, we collected approximately 10k patient-physician conversations from the online medical consultation website iCliniq2 to evaluate the performance of our model.  \\n\\nFor Q&A in medical scenarios, we collected and compiled a database, partly sampled in Fig. 3, which includes about 700 diseases and their associated symptoms, further medical tests or measures, and recommended medications, as a gold standard for the medical profession. \\n\\nThe database can be updated at any time without retraining the model and can the- oretically be set up for a specific disease database depending on the department or specific target.\\n\\ncan refer to the disease databases and Wikipedia (or any other reliable information source) as external knowledge brains of our ChatDoctor. \\n\\nAfter constructing the external knowledge brain, we need to let our ChatDoctor retrieve the knowledge he needs autonomously, which can generally be achieved in a large language model by constructing appropriate prompts\\n\\nTo automate this process, we design keyword mining prompts (Fig. 4) for ChatDoc- tor to extract key terms for relevant knowledge seeking. \\n\\nThen, the top-ranked relevant passages were retrieved from Knowledge Brain with a term-matching retrieval system. \\n\\nAs for the disease database, since the model cannot read all the data at once, we first let the model read the data in batches and select for itself the data entries that might help answer the patient’s question ' metadata={'source': 'llm_papers/chat-doctor-paper-summary.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Loading the documents from a directory\n",
    "path = './llm_papers'\n",
    "loader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c05bce",
   "metadata": {},
   "source": [
    "### Now that we have the documents with us, we do some simple prompting to ask questions of the LLM through Chains.  Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.\n",
    "\n",
    "### Before that, we also need to do some standard operations - text splitting, embedding creation and indexing them in a vector store.\n",
    "\n",
    "### Often times your document is too long (like a book) for your LLM. You need to split it up into chunks. Text splitters help with this.There are many ways you could split your text into chunks, experiment with different ones to see which is best for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fde6a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking the documents (splitting text into fixed size chunks)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf96d37",
   "metadata": {},
   "source": [
    "\n",
    "### Embeddings create a vector representation of a piece of text. This represents text in semantic or meaning space, so that pieces of text similar in meaning are near to each other in that space. \n",
    "\n",
    "### Vector databases provide an easy way to store the embedded text and perform the similarity search. We use Chromadb as the vector store here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# vectorising documents based on these embeddings\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# creating a retrieval chain\n",
    "llm_papers = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b54138ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The REACT model is an approach for improving the synergy between verbal reasoning and interactive decision making in autonomous systems. It allows the model to generate reasoning traces and task-specific actions in an interleaved manner, enabling it to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting while also interacting with the external environments to incorporate additional information into reasoning.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Question answering over Confluence data\n",
    "query = \"What is the REACT model?\"\n",
    "llm_papers.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a5de004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The ChatDoctor paper works in the medical domain as it is based on 100k real-world patient-physician conversations from an online medical consultation site and it also has autonomous knowledge retrieval capabilities.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which paper works in medical domain?\"\n",
    "llm_papers.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00141a2",
   "metadata": {},
   "source": [
    "### But we want more than question answering based on the document stores. We may want to get information which may not be existing in the document store. With langchain, this can be done with tools and agents. Tools can interact with different sources of information and an agent can create the context from document store and decide which tool to use and how to use it with the given context.\n",
    "\n",
    "### Below I create a custom search tool which is an combination of two tools - one searches the  document store to answer the user query, and the other is the search engine result page (serp) api tool, which I want to use whenever some question cannot be answered with document store data.\n",
    "\n",
    "### Langchain Tool class takes in three parameters - name of the tool, the functionality of the tool and a description of the tool, which can be useful for an agent to decide when and how to use the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9e82046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# defining search tool using SerpAPIWrapper\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "search_tool_default = [\n",
    "    Tool(\n",
    "        name = \"Document Store\",\n",
    "        func = llm_papers.run,\n",
    "        description = \"Use it to lookup information from the document store\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"Use this to lookup information from google search engine\",\n",
    "    )]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee04009",
   "metadata": {},
   "source": [
    "### Now we create an agent which will use the custom combined tool. We create a React zero shot agent, as we also get to see the reasoning behind sub-tool selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0fce4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should look for a paper that is related to AI and REACT.\n",
      "Action: Search\n",
      "Action Input: \"REACT paper in AI\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mIn this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The paper looks relevant, I should look for the authors.\n",
      "Action: Search\n",
      "Action Input: \"REACT paper in AI authors\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022). React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the authors of the REACT paper in AI.\n",
      "Final Answer: The authors of the REACT paper in AI are Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The authors of the REACT paper in AI are Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022).'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an agent an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(search_tool_default , llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"What is the REACT paper in AI about? Who are its authors?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abab3ed",
   "metadata": {},
   "source": [
    "### In the above result, we see that the agent takes the google search action, i.e. uses the SerpAPI tool all the time. We would like it to first lookup for information in the Document store first, before going to google.\n",
    "\n",
    "### So we specify this in the tool description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ef663b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new tool with order of tool usage made explicit\n",
    "\n",
    "search_tool_specify_order_in_dscrptn = [\n",
    "    Tool(\n",
    "        name = \"Document Store\",\n",
    "        func = llm_papers.run,\n",
    "        description = \"Use it to lookup information from document store. Always used as first tool\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"Use this to lookup information from google search engine. \\\n",
    "        Use it only after you have tried using the document store tool.\",\n",
    "    )]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a790ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find a document that contains the answer to this question.\n",
      "Action: Document Store\n",
      "Action Input: REACT paper in AI\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m The paper \"ReAct: Synergizing Reasoning and Acting in Language Models\" proposes a new approach for combining verbal reasoning with interactive decision making in autonomous systems. It explores the use of large language models (LLMs) to generate both reasoning traces and task-specific actions in an interleaved manner in order to improve the synergy between the two. The approach demonstrates how large language models can be used to address the limitations of traditional AI models and enable more reactive and adaptable reasoning in autonomous systems.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out who the authors are.\n",
      "Action: Search\n",
      "Action Input: authors of REACT paper in AI\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mDownload a PDF of the paper titled ReAct: Synergizing Reasoning and Acting in Language Models, by Shunyu Yao and 6 other authors. PDF · Other ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The authors of the REACT paper in AI are Shunyu Yao and 6 other authors.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The authors of the REACT paper in AI are Shunyu Yao and 6 other authors.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an agent which will use the modified tool\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(search_tool_specify_order_in_dscrptn , llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "# agent.agent.llm_chain.verbose=True\n",
    "# Now let's test it out!\n",
    "agent.run(\"What is the REACT paper in AI about? Who are its authors?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e9880",
   "metadata": {},
   "source": [
    "### This time the agent used the Document Store tool as the first action. Lets use it to answer more questions to see if it is consistent in following the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d91f39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should start by finding the paper in the document store.\n",
      "Action: Document Store\n",
      "Action Input: Medical domain paper\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m This is a medical domain paper that describes a fine-tuned model based on 100k real-world patient-physician conversations from an online medical consultation site. It also has autonomous knowledge retrieval capabilities, such as Wikipedia or a database as a knowledge brain. The data was collected from two online medical consultation websites, HealthCareMagic1 and iCliniq2.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should now look for the authors' affiliations.\n",
      "Action: Search\n",
      "Action Input: Authors of medical domain paper\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAuthors and Contributors in Scientific and Medical Publications · Step 1: Establish an authorship working group early in the trial, · Step 2: ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should look up the authors' affiliations from the paper.\n",
      "Action: Document Store\n",
      "Action Input: Authors of medical domain paper\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m The authors of the medical domain paper are not specified in the given context.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The authors' affiliations of the medical domain paper are not specified.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The authors' affiliations of the medical domain paper are not specified.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Which paper works in the medical domain? Which university are its authors affiliated to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c2d2117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should look up the paper to get more information\n",
      "Action: Document Store\n",
      "Action Input: Camel paper in LLMs\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m The authors of the paper discuss how ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner in order to enable dynamic reasoning and to incorporate additional information into reasoning.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should search for more information on the paper\n",
      "Action: Search\n",
      "Action Input: Summary of Camel paper in LLMs\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mCAMEL: Communicative Agents for \"Mind\" Exploration of LLMs - Summary. Summary: The paper proposes a novel communicative agent framework named role-playing to facilitate autonomous cooperation among communicative agents and provide insight into their “cognitive” processes.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The paper proposes a novel communicative agent framework named role-playing to facilitate autonomous cooperation among communicative agents and provide insight into their “cognitive” processes. It discusses how ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner in order to enable dynamic reasoning and to incorporate additional information into reasoning.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The paper proposes a novel communicative agent framework named role-playing to facilitate autonomous cooperation among communicative agents and provide insight into their “cognitive” processes. It discusses how ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner in order to enable dynamic reasoning and to incorporate additional information into reasoning.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " agent.run(\"What is this new Camel paper in LLMs about. Give detailed summary of the paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e15d4",
   "metadata": {},
   "source": [
    "### We see that the agent is not consistent in following the order of tools specified in the tool description. This is because the final prompt which is sent to the model does not explicitly mention this order constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1f33a",
   "metadata": {},
   "source": [
    "### We need to make changes in the final prompt which is being sent to the model. This can be seen using 'agent.agent.llm_chain.prompt.template'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf9f8a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Document Store: Use it to lookup information from the document store\n",
      "Search: Use this to lookup information from google search engine\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Document Store, Search]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "# let see what is final prompt which is being sent to the LLM\n",
    "\n",
    "print(agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82089c8",
   "metadata": {},
   "source": [
    "### We see that although the tool description mentions that the agent should use Document Store as the first tool, it is still not sticking to this rule. We can instead make this rule explicit in the React method description part of the prompt. Specifically, we can mention it during the Action component of React."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f92cb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent.llm_chain.prompt.template = '''\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "Document Store: Use it to lookup information from document store. Always used as first tool\n",
    "Search: Use this to lookup information from google search engine. Use it only after you have tried using the document store tool.\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [Document Store, Search]. Always look first in Document Store.\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc4ad93",
   "metadata": {},
   "source": [
    "### Now let's see if making the small and simple change above can make the agent follow our tool order rule better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f27041e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should try to find this information in the Document Store.\n",
      "Action: Document Store\n",
      "Action Input: REACT paper in AI\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m The ReAct paper is a research paper that explores the use of large language models to improve the synergy between verbal reasoning and interactive decision making in autonomous systems. It describes a model that allows for dynamic reasoning to create, maintain, and adjust high-level plans for acting while also interacting with the external environment to incorporate additional information into reasoning.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should now try to find out who the authors are.\n",
      "Action: Search\n",
      "Action Input: REACT paper in AI authors\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022). React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the authors of the REACT paper in AI.\n",
      "Final Answer: The authors of the REACT paper in AI are Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The authors of the REACT paper in AI are Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022).'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is the REACT paper in AI about? Who are its authors?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b488831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find a paper that works in the medical domain and find out the university affiliation of its authors.\n",
      "Action: Document Store\n",
      "Action Input: Medical domain paper\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m This is a medical domain paper about ChatDoctor, a fine-tuned model based on 100k real-world patient-physician conversations from an online medical consultation site. It also has autonomous knowledge retrieval capabilities, such as Wikipedia or a database as a knowledge brain. The paper also collected about 100k real doctor-patient conversations from an online medical consultation website HealthCareMagic-100k, and approximately 10k conversations from iCliniq2 to evaluate the performance of the model.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to find out the university affiliation of the authors.\n",
      "Action: Search\n",
      "Action Input: ChatDoctor authors\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge. Authors:Yunxiang Li, Zihan ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: The authors of the ChatDoctor paper are Yunxiang Li, Zihan Wang, and Xiang Ren from the University of Southern California.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The authors of the ChatDoctor paper are Yunxiang Li, Zihan Wang, and Xiang Ren from the University of Southern California.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Which paper works in the medical domain? Which university are its authors affiliated to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a76d6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should first look up the paper in the Document Store to get more information.\n",
      "Action: Document Store\n",
      "Action Input: Camel paper in LLMs\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which enables the model to perform dynamic reasoning and incorporate additional information into reasoning.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should now search for more information about the paper.\n",
      "Action: Search\n",
      "Action Input: Camel paper in LLMs\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThis paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: This paper looks at the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and robots. It proposes a novel learning paradigm called ReAct, which enables robots to reason dynamically and incorporate additional information into their reasoning.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This paper looks at the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and robots. It proposes a novel learning paradigm called ReAct, which enables robots to reason dynamically and incorporate additional information into their reasoning.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets query about a topic which doesn't exist in the document store.\n",
    "agent.run(\"What is this new Camel paper in LLMs about. Give detailed summary of the paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da66d9",
   "metadata": {},
   "source": [
    "### We now find the agent is more consistent in following the order of tools - firstly looking up the answer in the document store and only going to google search if it doesn't find anything with the first tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847e225",
   "metadata": {},
   "source": [
    "### There are still some problems with the final responses we get, especially the author names for the chat doctor paper. That can be addressed by analysing and processing the response we get from the serp-api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8813616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
